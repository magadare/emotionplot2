{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "749ae1b2-2382-4396-aac0-64bc5fba3ad4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-11T14:35:50.887031Z",
     "iopub.status.busy": "2025-06-11T14:35:50.886172Z",
     "iopub.status.idle": "2025-06-11T14:36:03.006112Z",
     "shell.execute_reply": "2025-06-11T14:36:03.004278Z",
     "shell.execute_reply.started": "2025-06-11T14:35:50.886964Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:59.469853: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749652559.491198  149937 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749652559.496714  149937 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1749652559.511908  149937 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749652559.512007  149937 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749652559.512009  149937 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1749652559.512011  149937 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-06-11 16:35:59.517429: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "# Model names\n",
    "FAST_MODEL = \"joeddav/distilbert-base-uncased-go-emotions-student\"\n",
    "ACCURATE_MODEL = \"SamLowe/roberta-base-go_emotions\"\n",
    "\n",
    "# Load both tokenizers and models\n",
    "tokenizers = {\n",
    "    \"fast\": AutoTokenizer.from_pretrained(FAST_MODEL),\n",
    "    \"accurate\": AutoTokenizer.from_pretrained(ACCURATE_MODEL)\n",
    "}\n",
    "\n",
    "models = {\n",
    "    \"fast\": AutoModelForSequenceClassification.from_pretrained(FAST_MODEL),\n",
    "    \"accurate\": AutoModelForSequenceClassification.from_pretrained(ACCURATE_MODEL)\n",
    "}\n",
    "\n",
    "# Move models to device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "for model in models.values():\n",
    "    model.to(device).eval()\n",
    "\n",
    "# Label maps (identical between the two)\n",
    "id2label = models[\"fast\"].config.id2label\n",
    "\n",
    "def predict_emotions(df, text_column=\"chunk\", top_k=3, batch_size=32, model_type=\"fast\"):\n",
    "    print(f\"[predict_emotions] Using model: {model_type}\")\n",
    "\n",
    "    model = models[model_type]\n",
    "    tokenizer = tokenizers[model_type]\n",
    "\n",
    "    texts = df[text_column].tolist()\n",
    "    all_probs = []\n",
    "\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        inputs = tokenizer(\n",
    "            batch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=512\n",
    "        )\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        all_probs.append(probs.cpu())\n",
    "\n",
    "    probs = torch.cat(all_probs, dim=0)\n",
    "\n",
    "    top_emotions = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for row in probs:\n",
    "        top_indices = torch.topk(row, top_k).indices.tolist()\n",
    "        top_scores = [round(row[i].item(), 3) for i in top_indices]\n",
    "        top_labels = [id2label[i] for i in top_indices]\n",
    "\n",
    "        top_emotions.append(dict(zip(top_labels, top_scores)))\n",
    "        predicted_labels.append(top_labels[0])\n",
    "\n",
    "    df[\"Predicted_Emotion\"] = predicted_labels\n",
    "    df[\"Top_3_Emotions\"] = top_emotions\n",
    "\n",
    "    print(\"[predict_emotions] Prediction complete.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a7bf58-4eea-4b72-8117-9644f04bd09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_emotions(df, text_column=\"chunk\", top_k=3, batch_size=32, model_type=\"accurate\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
